{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Define the model path for Pegasus Large\n",
    "model_paths = {\n",
    "    #  \"T5 Large\": \"t5-large\",\n",
    "    # \"LED Base\": \"allenai/led-base-16384\",\n",
    "    # \"mBART Large\": \"facebook/mbart-large-cc25\",\n",
    "    #   \"DialoGPT Large\": \"microsoft/DialoGPT-large\",\n",
    "    \"BART CNN Samsum\": \"philschmid/bart-large-cnn-samsum\",\n",
    "    \"BLOOM 560M\": \"bigscience/bloom-560m\"\n",
    "}\n",
    "\n",
    "# Function to load model and tokenizer\n",
    "def load_model(model_name):\n",
    "    print(f\"Loading {model_name} model...\")\n",
    "    # Use AutoModelForSeq2SeqLM specifically for Pegasus, which is a seq2seq model.\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_paths[model_name])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_paths[model_name], use_fast=False)  # Disable fast tokenizer for Pegasus and others\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load all models\n",
    "models = {name: load_model(name) for name in model_paths}\n",
    "\n",
    "def create_prompt():\n",
    "    \"\"\"\n",
    "    Creates a zero-shot prompt with the provided case text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The formatted prompt.\n",
    "    \"\"\"\n",
    "    enter_case = \"\"\"For context, this happened in 2020 during the pandemic, and I never knew the names of anyone here. \n",
    "Back in 2020, I got sick (not with covid) and I went to my doctor to get tested for strep. It was all going normal at first. \n",
    "I walked in, put on a mask, checked in, and sat down to wait. While I was on my phone, I see a male Karen, looking to be in his 40s or 50s maybe, \n",
    "come through the doors without a mask. The polite nurse tells him, \"Sir please put a mask on,\" but he refuses. Instead, he starts rambling on about how he has breathing problems. \n",
    "Again, the nurse steps in and informs this guy that breathing trouble can be a sign of covid. He then proceeds to start violently screaming at the nurse and cussing her out. \n",
    "While all this is happening, I am sitting a short distance away. Being the only other in the room, the nurse gives me a desperate look, but socially awkward me just stayed where I was. \n",
    "About 5 minutes later, 2 other nurses get this guy kicked out and I never saw him come back in. So, was I the jerk for doing nothing for the poor nurse?\n",
    "\"\"\"\n",
    "    prompt = f'Summarize the case text using this template as accurately as possible while maintaining correct English grammar. Do not add extra information: ' \\\n",
    "             f'The active agent did action to passive agent which led to consequence. The active agent had good/bad/neutral moral intention, ' \\\n",
    "             f'however, the action violated ethical principle which caused ethical issue. Case text is as follows: \"{enter_case}\"'\n",
    "    return prompt\n",
    "\n",
    "def perform_inference(model, tokenizer, prompt):\n",
    "    \"\"\"\n",
    "    Performs inference using the given model and prompt, setting the attention mask explicitly.\n",
    "    \n",
    "    Args:\n",
    "        model: The pre-trained model to perform inference.\n",
    "        tokenizer: The tokenizer to process the input.\n",
    "        prompt (str): The formatted prompt string.\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's generated answer.\n",
    "        float: Time taken for inference.\n",
    "    \"\"\"\n",
    "    # Tokenize the input and set attention mask\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)  # Max length increased\n",
    "    attention_mask = inputs['attention_mask']  # Extract attention mask\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    output = model.generate(inputs['input_ids'], attention_mask=attention_mask, max_new_tokens=200)  # Generating up to 200 new tokens\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Decode the generated output\n",
    "    inference_time = end_time - start_time\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response, inference_time\n",
    "\n",
    "def run_tests(models):\n",
    "    \"\"\"\n",
    "    Runs the inference tests on the given models and dataset with different prompt styles.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Dictionary containing the models and tokenizers.\n",
    "    \"\"\"\n",
    "    # Loop through each model\n",
    "    for model_name, (model, tokenizer) in models.items():\n",
    "        print(f\"\\nTesting with {model_name}...\\n\")\n",
    "        \n",
    "        # Create the prompt using the function\n",
    "        prompt = create_prompt()\n",
    "        response, time_taken = perform_inference(model, tokenizer, prompt)\n",
    "        print(f\"Zero-Shot Response: {response}\")\n",
    "        print(f\"Zero-Shot Inference Time: {time_taken:.4f} seconds\\n\")\n",
    "\n",
    "# Run tests\n",
    "run_tests(models)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
