{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install together -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def excel_to_json(excel_file, json_file):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(excel_file)\n",
    "    \n",
    "    # Convert DataFrame to a dictionary\n",
    "    data_dict = df.to_dict(orient='records')\n",
    "    \n",
    "    # Write dictionary to a JSON file\n",
    "    with open(json_file, 'w') as jsonf:\n",
    "        json.dump(data_dict, jsonf, indent=4)\n",
    "\n",
    "# Example usage\n",
    "excel_file = '.xlsx'  # Replace with your Excel file name\n",
    "json_file = '.json'   # Output JSON file name\n",
    "excel_to_json(excel_file, json_file)\n",
    "\n",
    "print(f\"Excel file '{excel_file}' has been converted to JSON file '{json_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from together import Together\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define the prompt creation function for summarizing cases\n",
    "def generate_case_summary_prompt(case_text):\n",
    "    return f\"\"\"\n",
    "   \n",
    "    Summarize the case text using this template as accurately as possible while\n",
    "    maintaining correct English grammar. Do not add extra information:\n",
    "    \"The <active agent> did <action> to <passive agent> which led to\n",
    "    <consequence>. The <active agent> had <good/bad/neutral> moral intention,\n",
    "    however, the <action> violated <ethical principle> ethical principle which\n",
    "    caused <ethical issue>.\"\n",
    "    Case text is as follows: \"{case_text}\"\n",
    "\n",
    "    give the output in commam seperated format\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "# Function to extract summary text from the raw API response\n",
    "def extract_summary_from_response(response_text):\n",
    "    try:\n",
    "        # Match the summary pattern starting with \"The\" and capturing the template\n",
    "        summary_match = re.search(r'^The .*', response_text, re.DOTALL)\n",
    "        if summary_match:\n",
    "            return summary_match.group().strip()  # Extract the summary text\n",
    "        else:\n",
    "            print(\"No valid summary found in response.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting summary: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to run the agent and fetch a response\n",
    "def run_agent(client, prompt, model, content):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"assistant\", \"content\": content},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        # Extract the content of the response\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        print(f\"Raw Response: {response_text}\")  # Log raw response\n",
    "        return extract_summary_from_response(response_text)  # Extract summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error in API call: {e}\")\n",
    "        return None\n",
    "\n",
    "# Set API key for Together client\n",
    "os.environ['TOGETHER_API_KEY'] = \"Your_API_Key\"\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "\n",
    "# Define the LLM model\n",
    "llm_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_json(\"practice.json\")\n",
    "\n",
    "# Initialize lists to collect generated summaries and invalid responses\n",
    "all_summaries = []\n",
    "invalid_responses = []\n",
    "\n",
    "# Process each case in the dataset\n",
    "for i in tqdm(range(len(data))):  # Iterate over all rows in the dataset\n",
    "    case_text = data.iloc[i][\"selftext\"]  # Extract the case text\n",
    "\n",
    "    # Generate a summary for the case text\n",
    "    summary = run_agent(\n",
    "        client,\n",
    "        generate_case_summary_prompt(case_text),\n",
    "        llm_model,\n",
    "        \"You are a legal domain expert generating case summaries.\"\n",
    "    )\n",
    "\n",
    "    # Validate and append the response\n",
    "    if summary:\n",
    "        all_summaries.append({\"case_index\": i, \"summary\": summary})  # Add summary\n",
    "    else:\n",
    "        invalid_responses.append({\"case_index\": i, \"case_text\": case_text})  # Add invalid response\n",
    "\n",
    "# Save all generated summaries to a CSV file\n",
    "if all_summaries:\n",
    "    summary_df = pd.DataFrame(all_summaries)\n",
    "    summary_output_file = \"summary.csv\"\n",
    "    summary_df.to_csv(summary_output_file, index=False)\n",
    "    print(f\"Generated summaries saved as '{summary_output_file}'\")\n",
    "else:\n",
    "    print(\"No valid summaries generated.\")\n",
    "\n",
    "# Save invalid responses for debugging\n",
    "if invalid_responses:\n",
    "    invalid_output_file = \"/kaggle/working/invalid_responses.json\"\n",
    "    with open(invalid_output_file, \"w\") as f:\n",
    "        json.dump(invalid_responses, f, indent=4)\n",
    "    print(f\"Invalid responses saved as '{invalid_output_file}'\")\n",
    "else:\n",
    "    print(\"No invalid responses.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
